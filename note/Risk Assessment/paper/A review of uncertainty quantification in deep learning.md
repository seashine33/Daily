# 深度学习中的不确定性量化综述：技术、应用和挑战

## 1 介绍
不确定性优化(UQ)

不确定性主要有两个来源：随机不确定性(aleatoric uncertainties)和认知不确定性(epistemic uncertainties)。  

[15]总结并分类了数据驱动优化范式在不确定性下的主要贡献。  
本文主要对数据驱动的优化进行了综述。  
[16]回顾了iyu神经网络的UQ。坐着专注于概率预测和预测区间（PIs）。

本文主要贡献：
> • 据我们所知，这是第一篇关于ML和DL方法中使用的UQ方法的全面综述论文，对该领域的研究人员来说是有价值的。  
> • 对新提出的UQ方法进行了全面审查。  
> • 此外，还列出了UQ方法重要应用的主要类别。  
> • 指出了UQ方法的主要研究空白。  
> • 最后，很少讨论未来的具体方向。  

## 2 背景知识
在本节中，我们解释了前馈神经网络的结构，然后进行贝叶斯建模，以详细讨论不确定性。  
### 2.1 前馈神经网络

### 2.2 不确定性建模
如上所述，存在两种主要的不确定性：认知的(模型不确定性)和随机的(数据不确定性)[18]。  
任意不确定性有两种类型：同方差(homoscedastic)和异方差(heteroscedastic)[19]。  
预测不确定性(PU)由两部分组成：(i)认识不确定性(EU)，(ii)算术不确定性(AU)，可以写成这两部分的总和：PU = EU + AU。

认知不确定性可以公式化为模型参数上的概率分布。
对于分类，可以使用**softmax似然**，并且可以假设**高斯似然**用于回归，通过应用**贝叶斯定理**，给定数据集Dtr在ω上的后验分布，然后预测。[对应四个公式]。
这个过程被称为推理或边缘化。然而，p（ω|X，Y）不能解析计算，但可以用变分参数来近似，即qθ（ω）。

其目的是近似于通过该模型获得的后验分布。因此，需要最小化Kullback-Leibler（KL）[20]关于θ的发散。

预测分布可以通过最小化KL发散来近似

KL散度最小化也可以重新排列为证据下界（ELBO）最大化[21]：

为了获得与数据相关的不确定性，（6）中的精度τ可以公式化为数据的函数。获得认识不确定性的一种方法是混合两个函数：预测均值，即fθ（x）和模型精度，即gθ（x。将先验分布放置在模型的权重上，然后计算给定数据样本的权重变化量。
## 3 基于贝叶斯技术的不确定性量化
### 3.1 贝叶斯深度学习/贝叶斯神经网络
尽管标准DL方法在解决各种实际单词问题方面取得了成功，但它们无法提供有关其预测可靠性的信息。为了缓解这个问题，贝叶斯深度学习（BDL）/贝叶斯神经网络（BNN）[24]、[25]、[26]、[27]、[28]、[29]、[30]、[31]可以用于解释模型参数。BNN/BDL对过拟合问题具有鲁棒性，可以在小数据集和大数据集上进行训练[32]。  


