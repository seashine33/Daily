# 深度学习中的不确定性量化综述：技术、应用和挑战

## 1 介绍
不确定性优化(UQ)

不确定性主要有两个来源：随机不确定性(aleatoric uncertainties)和认知不确定性(epistemic uncertainties)。  

[15]总结并分类了数据驱动优化范式在不确定性下的主要贡献。  
本文主要对数据驱动的优化进行了综述。  
[16]回顾了iyu神经网络的UQ。坐着专注于概率预测和预测区间（PIs）。

本文主要贡献：
> • 据我们所知，这是第一篇关于ML和DL方法中使用的UQ方法的全面综述论文，对该领域的研究人员来说是有价值的。  
> • 对新提出的UQ方法进行了全面审查。  
> • 此外，还列出了UQ方法重要应用的主要类别。  
> • 指出了UQ方法的主要研究空白。  
> • 最后，很少讨论未来的具体方向。  

## 2 背景知识
在本节中，我们解释了前馈神经网络的结构，然后进行贝叶斯建模，以详细讨论不确定性。  
### 2.1 前馈神经网络

### 2.2 不确定性建模
如上所述，存在两种主要的不确定性：认知的(epistemic, 模型不确定性)和随机的(aleatoric, 数据不确定性)[18]。  
**随机不确定性**有两种类型：同方差(homoscedastic)和异方差(heteroscedastic)[19]。

**预测不确定性**(predictive uncertainty)由两部分组成：(i)认识不确定性(epistemic, EU)，(ii)随机不确定性(aleatoric, AU).  
可以写成这两部分的总和: PU = EU + AU。

**认知不确定性**(epistemic uncertainty)可以公式化为模型参数上的概率分布。  
设Dtr={X，Y}={(xi，yi)}表示一个训练数据集, 其目的是优化能够产生所需输出的函数y=fω（x）的参数，即ω。为了实现这一点，贝叶斯方法定义了一个**model likelihood, 模型似然性**，即p(y|x,ω)。  
- 公式(5)到(8), 是一段公式推导：
- 对于分类，可以使用**softmax likelihood**，p(y=c|x, ω) =  
- 并且可以假设**Gaussian likelihood**用于回归，p(y|x, ω) =  
- 通过应用[贝叶斯定理](./Bayes'%20theorem.md)，给定数据集Dtr在ω上的后验分布,p(ω|X, Y) =  
- 然后预测，p(y*|x*, X, Y) =  

这个过程被称为推理或边缘化。然而，p(ω|X, Y)不能解析计算，但可以用变分参数来近似，即qθ(ω)。  
其目的是对通过该模型获得的后验分布(posterior distribution)进行近似。  
因此，需要使Kullback-Leibler(KL)[20]的发散度相对于θ最小化。  
公式(9)用于评判两个分布之间的相似性。

预测分布可以通过最小化KL发散来近似，即公式(10)。

KL散度最小化也可以重新排列为**证据下界**（ELBO）最大化[21]，即公式(11).

为了获得与**数据相关的不确定性**，（6）中的精度τ可以公式化为数据的函数。  
获得**认识不确定性**的一种方法是混合两个函数：预测均值，即fθ（x）和模型精度，即gθ(x)，其似然函数可以表示为yi = N(fθ(x), gθ(x)^−1)。将先验分布放置在模型的权重上，然后计算给定数据样本的权重变化量。

## 3 基于贝叶斯技术的不确定性量化
### 3.1 贝叶斯深度学习/贝叶斯神经网络
尽管标准DL方法在解决各种实际单词问题方面取得了成功，但它们无法提供有关其**预测可靠性**的信息。为了缓解这个问题，贝叶斯深度学习（BDL）/贝叶斯神经网络（BNN）[24]、[25]、[26]、[27]、[28]、[29]、[30]、[31]可以用于解释模型参数。BNN/BDL对过拟合问题具有鲁棒性，可以在小数据集和大数据集上进行训练[32]。  

### 3.2 蒙特卡罗(MC) dropout
如前所述，很难计算精确的后验推理，但它可以近似。在这方面，蒙特卡罗（MC）[33]是一种有效的方法。尽管如此，当集成到深度架构中时，这是一种缓慢且计算成本高昂的方法。为了解决这一问题，引入了MC（MC）丢弃，它使用丢弃[34]作为正则化项来计算预测不确定性[35]。Dropout是一种有效的技术，已被广泛用于解决DNN中的过拟合问题。在训练过程中，dropout随机丢弃NN的一些单元，以避免它们过多地协同调整。假设NN具有L个层，其中Wl、bl和Kl分别表示第L层的权重矩阵、偏差向量和维度。第i个输入xi（i=1，…，N）的NN和目标类的输出分别由Şyi和yi表示。使用L2正则化的目标函数可以写成：


### 3.3 马尔可夫链蒙特卡罗(MCMC)

### 3.4 变分推理(VI)

### 3.5 贝叶斯主动学习(BAL)

### 3.6 Bayes by Backprop (BBB)

### 3.7 可变自动编码器(Variational Autoencoders)

## 4 其他方法

### 4.1 深高斯过程(Deep Gaussian Processes)

### 4.2 拉普拉斯近似(Laplace approximations)

## 5 强化学习中的不确定性量化

## 6 技术集成

## 7 UQ方法的进一步研究

## 8 应用

## 9 文献空白与未决问题